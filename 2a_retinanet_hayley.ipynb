{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "643e283a",
   "metadata": {},
   "source": [
    "## 1. imports + environment check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ded9494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.12.7 | packaged by conda-forge | (main, Oct  4 2024, 15:47:54) [MSC v.1941 64 bit (AMD64)]\n",
      "Executable: c:\\Users\\owner\\anaconda3\\python.exe\n",
      "Arch: ('64bit', 'WindowsPE')\n"
     ]
    }
   ],
   "source": [
    "import sys, platform\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"Executable:\", sys.executable)\n",
    "print(\"Arch:\", platform.architecture())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb834cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pycocotools in c:\\users\\owner\\anaconda3\\lib\\site-packages (2.0.11)\n",
      "Requirement already satisfied: numpy in c:\\users\\owner\\anaconda3\\lib\\site-packages (from pycocotools) (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Pillow OK\n",
      "Torch: 2.9.1+cpu\n",
      "Torchvision: 0.24.1+cpu\n",
      "pycocotools OK ✅\n"
     ]
    }
   ],
   "source": [
    "%pip install pycocotools\n",
    "\n",
    "from PIL import Image\n",
    "import torch, torchvision\n",
    "from torchvision.datasets import CocoDetection\n",
    "from pycocotools.coco import COCO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cf73d5",
   "metadata": {},
   "source": [
    "## 2. paths + config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3883d6a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "ROOT = Path(\"signs_coco\")\n",
    "IMG_DIR = ROOT / \"images\"\n",
    "ANN_FILE = ROOT / \"result.json\"\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "device = torch.device(\"cpu\") \n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dace4e9",
   "metadata": {},
   "source": [
    "## 3. dataset + transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bc5ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Dropped attribute categories: ['Back', 'Circular', 'Damaged', 'Front', 'Good', 'Heavily Damaged', 'Octagonal', 'Pole-mounted', 'Side', 'Square', 'Triangular', 'Wall-mounted', 'Weathered']\n",
      "Number of sign-type classes: 25\n",
      "num_classes (incl background): 26\n",
      "Example idx: 0 boxes: 3 labels sample: [19, 22, 6]\n",
      "Example idx: 1 boxes: 2 labels sample: [22, 19]\n",
      "Example idx: 2 boxes: 2 labels sample: [19, 22]\n",
      "Found: 3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.datasets import CocoDetection\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "def transforms(img, target):\n",
    "    img = F.to_tensor(img)\n",
    "    return img, target\n",
    "\n",
    "ATTRIBUTE_NAMES = {\n",
    "    \"Front\", \"Side\", \"Back\",\n",
    "    \"Wall-mounted\", \"Pole-mounted\",\n",
    "    \"Good\", \"Weathered\", \"Heavily Damaged\", \"Damaged\",\n",
    "    \"Circular\", \"Square\", \"Triangular\", \"Octagonal\",\n",
    "}\n",
    "\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "class CocoForSignTypeDetection(CocoDetection):\n",
    "    def __init__(self, img_folder, ann_file, transforms=None):\n",
    "        super().__init__(img_folder, ann_file)\n",
    "        self._transforms = transforms\n",
    "        self.img_folder = Path(img_folder)\n",
    "\n",
    "        cats = self.coco.loadCats(self.coco.getCatIds())\n",
    "        cats = sorted(cats, key=lambda c: c[\"id\"])\n",
    "\n",
    "        sign_cats = [c for c in cats if c[\"name\"] not in ATTRIBUTE_NAMES]\n",
    "\n",
    "        self.cat_id_to_contig = {c[\"id\"]: i + 1 for i, c in enumerate(sign_cats)}\n",
    "        self.contig_to_name = {i + 1: c[\"name\"] for i, c in enumerate(sign_cats)}\n",
    "        self.num_classes = len(sign_cats) + 1\n",
    "\n",
    "        self._kept_names = [c[\"name\"] for c in sign_cats]\n",
    "        self._dropped_names = [c[\"name\"] for c in cats if c[\"name\"] in ATTRIBUTE_NAMES]\n",
    "\n",
    "    def _load_image(self, id: int):\n",
    "        # COCO json file_name may contain a Label Studio path; we only want the basename\n",
    "        file_name = self.coco.loadImgs(id)[0][\"file_name\"]\n",
    "        base = Path(file_name).name  # strips ../../label-studio/... and keeps IMG_xxx.jpg\n",
    "        img_path = self.img_folder / base\n",
    "        return Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        id = self.ids[idx]\n",
    "        img = self._load_image(id)\n",
    "        anns = self.coco.loadAnns(self.coco.getAnnIds(imgIds=id))\n",
    "\n",
    "        boxes, labels = [], []\n",
    "        for a in anns:\n",
    "            coco_cat_id = a[\"category_id\"]\n",
    "            if coco_cat_id not in self.cat_id_to_contig:\n",
    "                continue\n",
    "\n",
    "            x, y, w, h = a[\"bbox\"]\n",
    "            if w <= 1 or h <= 1:\n",
    "                continue\n",
    "            boxes.append([x, y, x + w, y + h])\n",
    "            labels.append(self.cat_id_to_contig[coco_cat_id])\n",
    "\n",
    "        target = {\n",
    "            \"boxes\": torch.tensor(boxes, dtype=torch.float32),\n",
    "            \"labels\": torch.tensor(labels, dtype=torch.int64),\n",
    "            \"image_id\": torch.tensor([id]),\n",
    "        }\n",
    "\n",
    "        if self._transforms:\n",
    "            img, target = self._transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "dataset = CocoForSignTypeDetection(str(IMG_DIR), str(ANN_FILE), transforms=transforms)\n",
    "\n",
    "print(\"Dropped attribute categories:\", sorted(dataset._dropped_names))\n",
    "print(\"Number of sign-type classes:\", dataset.num_classes - 1)\n",
    "print(\"num_classes (incl background):\", dataset.num_classes)\n",
    "\n",
    "found = 0\n",
    "for i in range(len(dataset)):\n",
    "    img, tgt = dataset[i]\n",
    "    if len(tgt[\"boxes\"]) > 0:\n",
    "        print(\"Example idx:\", i, \"boxes:\", len(tgt[\"boxes\"]), \"labels sample:\", tgt[\"labels\"][:5].tolist())\n",
    "        found += 1\n",
    "    if found == 3:\n",
    "        break\n",
    "\n",
    "print(\"Found:\", found)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ce88da",
   "metadata": {},
   "source": [
    "## 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "44c68cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 481 Val: 121\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Subset\n",
    "import random\n",
    "\n",
    "indices = list(range(len(dataset)))\n",
    "random.seed(42)\n",
    "random.shuffle(indices)\n",
    "\n",
    "split = int(0.8 * len(indices))\n",
    "train_ds = Subset(dataset, indices[:split])\n",
    "val_ds   = Subset(dataset, indices[split:])\n",
    "\n",
    "print(\"Train:\", len(train_ds), \"Val:\", len(val_ds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fd96ff",
   "metadata": {},
   "source": [
    "## 5. dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "af297e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "BATCH_SIZE = 2      \n",
    "NUM_WORKERS = 0     \n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "    num_workers=NUM_WORKERS, collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_ds, batch_size=1, shuffle=False,\n",
    "    num_workers=NUM_WORKERS, collate_fn=collate_fn\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0628c5ba",
   "metadata": {},
   "source": [
    "## 6. model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9a8db0c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model ready. Classes incl background: 26\n",
      "Head num_classes: 26\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection.retinanet import RetinaNet_ResNet50_FPN_Weights\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "weights = RetinaNet_ResNet50_FPN_Weights.DEFAULT\n",
    "\n",
    "model = torchvision.models.detection.retinanet_resnet50_fpn(weights=weights)\n",
    "\n",
    "from torchvision.models.detection.retinanet import RetinaNetClassificationHead\n",
    "num_anchors = model.head.classification_head.num_anchors\n",
    "\n",
    "first_block = model.head.classification_head.conv[0]\n",
    "conv0 = first_block[0]  \n",
    "in_channels = conv0.in_channels\n",
    "\n",
    "model.head.classification_head = RetinaNetClassificationHead(\n",
    "    in_channels=in_channels,\n",
    "    num_anchors=num_anchors,\n",
    "    num_classes=dataset.num_classes \n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "print(\"Model ready. Classes incl background:\", dataset.num_classes)\n",
    "print(\"Head num_classes:\", model.head.classification_head.num_classes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65f1d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9512416d",
   "metadata": {},
   "source": [
    "## training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30e6ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-batch training step OK ✅\n",
      "{'classification': 1.231806993484497, 'bbox_regression': 0.6752537488937378}\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "images, targets = next(iter(train_loader))\n",
    "images = [img.to(device) for img in images]\n",
    "targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "loss_dict = model(images, targets)\n",
    "loss = sum(loss_dict.values())\n",
    "\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "print({k: float(v.detach().cpu()) for k, v in loss_dict.items()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f546867d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 | avg loss 1.2795: 100%|██████████| 241/241 [1:09:41<00:00, 17.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | avg loss=1.2795 | time=4181.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 | avg loss 0.9173: 100%|██████████| 241/241 [1:08:25<00:00, 17.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 | avg loss=0.9173 | time=4105.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 | avg loss 0.8361: 100%|██████████| 241/241 [1:07:14<00:00, 16.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 | avg loss=0.8361 | time=4034.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 | avg loss 0.8902: 100%|██████████| 241/241 [1:07:56<00:00, 16.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 | avg loss=0.8902 | time=4076.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 | avg loss 0.7069: 100%|██████████| 241/241 [1:07:51<00:00, 16.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 | avg loss=0.7069 | time=4071.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 | avg loss 0.7315:  30%|██▉       | 72/241 [21:04<49:27, 17.56s/it]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 35\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, EPOCHS \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     34\u001b[0m     t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 35\u001b[0m     avg_loss \u001b[38;5;241m=\u001b[39m train_one_epoch(epoch)\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | avg loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | time=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;241m-\u001b[39mt0\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[27], line 19\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(epoch)\u001b[0m\n\u001b[0;32m     17\u001b[0m loss_dict \u001b[38;5;241m=\u001b[39m model(images, targets)\n\u001b[0;32m     18\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(loss_dict\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;241m/\u001b[39m ACCUM_STEPS\n\u001b[1;32m---> 19\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m ACCUM_STEPS \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     22\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\owner\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:625\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    616\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    617\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    618\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    623\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    624\u001b[0m     )\n\u001b[1;32m--> 625\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    626\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    627\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\owner\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    349\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    351\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 354\u001b[0m _engine_run_backward(\n\u001b[0;32m    355\u001b[0m     tensors,\n\u001b[0;32m    356\u001b[0m     grad_tensors_,\n\u001b[0;32m    357\u001b[0m     retain_graph,\n\u001b[0;32m    358\u001b[0m     create_graph,\n\u001b[0;32m    359\u001b[0m     inputs_tuple,\n\u001b[0;32m    360\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    361\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    362\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\owner\\anaconda3\\Lib\\site-packages\\torch\\autograd\\graph.py:841\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    839\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    840\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 841\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    842\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    843\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    844\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    845\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "EPOCHS = 10\n",
    "ACCUM_STEPS = 1  \n",
    "\n",
    "def train_one_epoch(epoch):\n",
    "    model.train()\n",
    "    running = 0.0\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    pbar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "    for step, (images, targets) in pbar:\n",
    "        images = [img.to(device) for img in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        loss = sum(loss_dict.values()) / ACCUM_STEPS\n",
    "        loss.backward()\n",
    "\n",
    "        if (step + 1) % ACCUM_STEPS == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        running += loss.item() * ACCUM_STEPS\n",
    "        pbar.set_description(f\"Epoch {epoch} | avg loss {running/(step+1):.4f}\")\n",
    "\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    return running / max(1, len(train_loader))\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    t0 = time.time()\n",
    "    avg_loss = train_one_epoch(epoch)\n",
    "    print(f\"Epoch {epoch}/{EPOCHS} | avg loss={avg_loss:.4f} | time={time.time()-t0:.1f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c2392e",
   "metadata": {},
   "source": [
    "## evaluation + qualitativ predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9592f2b3",
   "metadata": {},
   "source": [
    "## save weights + inference function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a684c622",
   "metadata": {},
   "source": [
    "## analytics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
